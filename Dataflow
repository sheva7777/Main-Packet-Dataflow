Block Name Description
NNI 网络-网络接口（Network-Network Interface）。这个接口为1 个SerDes 接
口，内部集成多个PON MAC 和1 个GE MAC，可以和10G xPON、xPON、
GE 的光收发器连接。
UNI 用户网络接口（User Network Interface）。支持4 个GE，和1 个RGMII 接
口。
IPS 入端口调度（Ingress Port Scheduler）。入口端口的汇聚和调度。
PQM 报文队列管理（Packet Queue Manager）。进行入口队列管理和调度，给PSA
提供报文头和数据；对于PSA处理完的报文，从PLB读出送给PE编辑。
HA 硬件加速器（Hardware Accelerator），分析报文头，匹配入队优先级，并生
成报文描述信息。
MEMP 存储池（Memory Pool）。报文处理延时缓存和片内缓存共享的存储器，此共
享存储池大小约8Mbit。
FAM
空闲地址管理器（Free Address Manager）。 分配和回收数据通路中所有报文使用的缓存地址。并进行组播ID和权重管理。
PE
报文编辑器（Packet Editor）。 此模块根据PSA产生的报文编辑命令和编辑数据，接收经过PQM读取的原始报文，进行报文编辑。
EQM
出口队列管理（Egress Queue Manager）。进行出口队列的管理和调度。
EPS
出口端口调度器（Egress Port Scheduler）。负责读取要发送的报文数据并分发给指定的接口模块。
AP
执行处理器（Action Processor）。进行报文的分析和处理。
EP
编辑处理器（Edit Processor）。根据AP处理结果，产生报文编辑命令和报文编辑数据。
FP
Fine-Grain Processor。专职用于“控制处理”任务，如OAM、时戳处理等。仅仅由一个RAE构成。
OE
排序器（Order Enforcer）。 此模块学习每个端口报文的接收顺序，确保报文的发送顺序和接收顺序一致。同时此模块能确保一些访问协处理器命令按照顺序执行。
COP
协处理器（Co-Processor）。用于访问TCAM、内部通用memory以及外部通用memory。同时支持EM（Exact Match）和LPM（Longest Prefix Match）算法。
PIE
报文插入和提取（Packet Insert and Extract）。 给CPU提供向数据通路插入报文或从数据通路提取报文的方法。
WOE
Wifi Offloading Engine，此模块完成Wifi Offloading线路特性相关功能，如reorder、队列管理等。
ECS
嵌入CPU子系统（Embedded CPU Sub-system）。
ICSS
内部控制处理器子系统（Internal Control processor Sub-System）.。此模块让控制平面访问芯片内 的控制和状态空间。

为描述方便，将接口模块以外的模块划分到3 个子系统中：
1）数据通道子系统（Datapath Sub-System）: 主要完成帧数据的缓存、队列管理和QoS 调度功能。 属于数据通道子系统的主要模块有IPS、PQM、MEMP、FAM、PE、EQM、EPS 等模块。
2）业务处理子系统：主要由可编程查找执行模块（Programmable Search-Action）、HA模块构成，来完成报文协议分析、转发处理等功能。
3）内嵌控制平面子系统（Embedded Control Plane Sub-System）：提供内置控制CPU 和整个芯片的配置和寄存器访问。控制平面子系统包括ECS、ICSS 等模块。
MEMP 的片内作为报文缓存的资源分成两大部分：报文处理延迟缓冲区（PLB）和内部帧缓冲区（IFB）。信元大小统一为160B，Head 信元的报文头数据为128B，FD 信息为32B。

业务数据在芯片中的处理过程如下：
1. 系统所有入口数据由IPS汇聚：IPS先向FAM申请缓存，对所有输入接口进行调度，按照调度结果，将报文存入PLB（MEMP）。当一个完整报文存入后，将报文头读出，发送给HA模块。
2. HA对报文头数据进行解析，生成PSA处理需要的帧描述信息（FD），并根据报文特征匹配出入口队列优先级，然后将帧描述信息、入队优先级和报文指针发送给PQM模块。
3. PQM首先将帧描述信息存入PLB（MEMP），然后根据报文入队优先级，将报文（指针）入队和调度，在系统出现反压的时候，优先级低的队列首先进行丢包（并释放MEMP缓存）。当PSA有空闲线程，PQM调度输出，将相应的报文Head再次读出，输送给PSA处理。PQM支持pull More指令读取Head外的更多数据。
PQM还负责管理PAC_ID资源，当新报文Head发给PSA时，会申请并携带PAC_ID；当PE读走单播或者组播最后一份报文时，则释放PAC_ID 资源。
4. 在Quark的AE模块进行报文的各种分析和处理。
5. 当AE完成完成一个报文的处理后，发送Transmit*命令给OE，进行保序处理，当从“保序队列”输出时：
a. 指示PQM把报文读出，准备给PE编辑。
b. 同时指示EE读取报文context信息。
c. 在Context信息全部读出后，通知AE释放线程。
6. EE模块根据报文context信息生成编辑命令（EC）和编辑数据（ED），发送给PE。
7. 在PQM将报文送给PE之后，释放Head和Body的缓存。
8. PE模块接收并缓存从PSA送过来的EC和ED，对PQM送过来的报文数据，进行相应的报文编辑（并计算checksum）。PE还负责完成编辑后的报文头及报文身体的搬运和链接，所有报文首先搬运到内置缓存（MEMP）。然后将报文描述符送给EQM。
9. EQM首先进行入队判决，成功入队的报文存入“端口组队列”（按照出端口为NNI、UNI、PIE/PRBS分为3个队列），进行调度，将调度出的报文搬运到外置缓存（DDR）或者继续存储在内置缓存中，然后将报文描述符送给“出端口队列”，进行出口QoS管理。
10. EPS模块从内置或者外置缓存读取数据，完成出口数据分发，以及地址释放。



EQM模块内部维护一个MCID相应深度的组播Body地址表，用来存放EFB Body地址，同时这个表项有1bit标志位，用于标识这个MCID的Body地址是否已经记录过， MCID释放时，FAM通知将这个bit清0。完成通道组队列调度后，如果报文需要存放到EFB，且报文是组播报文，则释放1份MCID_IFB权重，增加1份MCID_EFB权重（MCID_EFB的初始值为0）；另外，判断这个报文的Body地址是否已经记录过，如果未记录过，将这个报文整包存放在EFB，并记录Body地址；如果这个报文的Body地址已经记录过，则仅把这个报文的Head存放到EFB，并用已经存放的Body地址建链。
9. EPS读取报文后，如果是组播报文，则进行权重释放（根据报文存储位置，确定MCID_IFB还是MCID_EFB）。

MEMORY POOL(MEMP)
Memory Pool（MEMP）是芯片内置的用于缓存报文的空间。数据通道中的多个处理环节分别需要大容量的内置Memory来临时存储报文数据，我们将这些Memory集中放置在MEMP模块，实现资源共享，通过灵活配置，应付各种应用场景。
MEMP在逻辑上分为PLB、PEB和IFB三部分缓存空间，其中PLB用于缓存入端口报文（PE前）；PEB用于缓存PE编辑之后、进出口队列之前的报文；IFB用于缓存 出端口队列报文（存于片内的部分）。
5 个读写模块完成3 写（IPS 写整包，PQM 写FD，PE 写整包或Head）5 读（IPS 读Head，PQM 读Head 或部分Body，PQM 读整包或Head，EQM 读整包或Head，EPS 读整包）操作，加上CPU 读写接口，共4 写6 读。读写仲裁模块（MEMP_RW_ARB）根据各个读写命令操作的地址，完成5 块SRAM的读写命令的分发，并配置到各个SRAM的读写端口上。MEMP 由5 个位宽128bit 的SRAM 组成，逻辑上每一行地址为80B，每二行设计为一个CELL（每个CELL 就是160B）。数据存储方式和FAM的地址分配相关，报文缓存在多个cell 时，由链表进行管理。

EQM 模块对IFB 的操作，一读。
一读：EQM 对出口Buffer 组调度，分析报文存储目的地，如果为EFB，则从IFB 读出。

EPS 模块对IFB 的操作，一读。
一读：读取整包报文数据，分发给目的接口。

PACKET EDITOR (PE)
PE 模块从Quark 接收EC（Edit Command）和ED（Edit Data），存储到内部的Buffer 中。PE 从PQM 接收原始报文数据，根据编辑命令（EC）对报文进行编辑。在申请到FAM（Free Address Manager）的缓存地址之后，将编辑完成的报文数据写入内部缓存IFB（Internal Frame Buffer）中，最后将报文的FCB（Frame Control Block）送给EQM（Egress Queue Manager）模块处理。

PE 模块的主要特性描述如下：
 顺序执行ECB（Edit Command Buffer）中的命令，命令类型如下：
o ADD：添加长度为0 ~48Bytes 的数据到原始报文中的指定位置；
o REPLACE：把0~128Bytes 的连续字段，替换成0~48Bytes 的新数据；
o KEEP：保持最多16383Bytes 数据的原始数据；
o REMOVE：删除最多16383Bytes 数据的原始数据。
 支持当编辑命令（Edit Command）没有覆盖整个原始报文数据的时候，根据Quark
过来的指示自动生成剩下原始报文的KEEP/REMOVE 命令。
o 当EDIT_INFO 的TXTOEND 标记无效的时候，对EC 命令没有覆盖的报文段，
全部删除；如果有TXTOEND 标记有效的时候，PE 保持剩下的报文传输。
 支持剥离报文最后4 字节。
当EDIT_INFO的REMOVE_LAST标记为1时删除报文的最后4个字节；当REMOVE_LAST标记为0时，不进行任何操作。
 支持IP头的TTL(Time To Live)/HL(Hop Limit)的修改。
 支持对TPID的修改，最多支持四层TAG同时修改。
 支持三组Checksum计算，分别是IPv4头、TCP/UDP和一组扩展的Checksum。
 支持对报文打上时戳。

下面的表格给出PE 支持的所有EC 命令和每个命令的命名。
Item Capacity
ADD [ADD_SIZE] 执行ADD 命令是将“ADD_SIZE”Bytes 的数据插到原始报文中；ADD_SIZE 的
范围是0~48 字节。插入的报文位置是由当前的指针指示，执行ADD 命令不会
移动指针。
REPLACE [REP_SIZE,
ADD_SIZE]
执行REPLACE 命令是先删除“REP_SIZE”Bytes 的原始报文数据，再添加
“ADD_SIZE”Bytes 的数据到原始报文中；REP_SIZE 的范围是0~128 字节，
ADD_SIZE 的范围是0~48 字节。插入的报文位置是由当前的指针指示，执行
REPLACE 命令会将指针向前移动REP_SIZE。
如果剩余的原始报文数据少于REP_SIZE，只有实际的原始报文长度被剥离。
KEEP
[KEEP_RM_SIZE]
执行KEEP 命令是保持“KEEP_RM_SIZE”Bytes 的原始报文数据；
KEEP_RM_SIZE 的范围是0~16383 字节。起始的位置是由当前的指针指示，执
行KEEP 命令会将指针向前移动KEEP_RM_SIZE。
如果剩余的原始报文数据少于KEEP_RM_SIZE，只有实际的原始报文长度被保
持。
REMOVE
[KEEP_RM_SIZE]
执行REMOVE 命令是删除“KEEP_RM_SIZE”Bytes 的原始报文数据；
KEEP_RM_SIZE 的范围是0~16383 字节。起始的位置是由当前的指针指示，执
行REMOVE 命令会将指针向前移动KEEP_RM_SIZE。
如果剩余的原始报文数据少于KEEP_RM_SIZE，只有实际的原始报文长度被删
除。

上图对报文的编辑操作进行了一个示例，报文编辑按照如下的方式进行：
 根据命令将L2 报文头全部删除；
 在IP 头前面添加了一个数据块A；
 将原来的IP Header Part1 头按照数据块B 的数据进行修改；
 IP Header Part 2 到报文的尾部，不做任何修改；
 在报文的尾部添加数据块C。


EGRESS QUEUE MANAGER(EQM )
Egress Queue Management（EQM）模块接收PE送过来的包信息，控制报文数据的存储，基于输出队列进行缓存管理和调度，完成PON口的入队DBA信息发送给EPS_PON。

PBM：Packet Buffer Manager,PBM子模块从PE或EPS模块接收报文信息，用于缓存管理。缓存管理包括：队列资源管理和WRED，播叶子丢弃，冗余权重释放，入队报文资源统计等。
PDM: 决策将报文数据存储在IFB或者EFB，若决策为EFB的报文，则将报文从IFB中读出并写入EFB中。
QE: 为每个队列维护链表信息：头指针、尾指针、空闲状态等。每次报文入队时，更新队列链表的尾指针；每次报文出队时，更新队列链表的头指针。同时，还根据入队和出队状态，维护队列链表的空闲状态。
QS：主要完成多级调度和Shaping 操作。

PE 的入队报文和EPS 的反馈的二次入队报文在EQM 首先是要做入队判决，成功入队的报文进行内外置缓存的选择，若是EFB 转发的报文需要将报文从IFB 搬到EFB 中，然后报文才进入正常的业务队列，并更新队列链表状态和空状态；如果是IFB 转发的报文，则报文不需要搬，直接更新队列状态。入队失败的报文，向FAM释放缓存并统计丢包。报文出队时，发出报文的FD 给EPS。如果报文是二次入队报文，在EPS 从DDR 获得FD信息后反馈报文信息给EQM 进行二次入队，非二次入队的报文EPS 会正常转发到端口上。

支持三种资源管理方式：基于报文个数管理、CELL 数管理、16BYTE 为单位的资源管理。报文数管理和CELL 管理支持全局可配置，要么配置成报文数管理，要么配置成CELL 管理,16B 缓存管理可以单独配置关闭。系统运行过程中BYTE 管理和CELL 管理（或包管理）同时运行。下面的描述适合上述三种管理方式。
 芯片的队列资源分为两部分:队列独享资源和共享资源。
 队列独享队列资源是为队列预留的队列资源，即该队列至少可用的队列资源数目，即使该队列不用，其它队列也不能占用。
 共享队列资源是为队列竞争使用的。一个报文申请入队，优先占用该队列的独享队列资源。如果独享队列资源已经用完，再申请占用共享资源。返还的时候，优先返还共享资源，之后返还独享资源。
 队列独享资源，共享资源之和必须小于等于芯片支持的最大队列资源数目。如果配置不正确，则每个队列的独享队列资源得不到保证。
 队列基于模板可配独享队列资源和最大队列长度。

队列组的划分及配置:
队列组是指一组队列，队列组的划分基于队列模板可配，即队列属于哪个队列组可配。
 具有队列组的属性，是指基于队列组的配置对缓存进行管理。
 软件可规划队列组的划分方法，逻辑不做限制。
 队列组可配队列组最大共享资源group_max。灵活配置，既可限制队列组的最大共享资源占用，又可达到保留共享资源给队列组使用的效果。

因为队列资源使用策略的配置，导致报文丢弃，称为尾丢弃。具体包括队列长度超过阈值，或者独享队列资源已经用完，申请不到共享队列资源。
当队列发生尾丢弃之后，什么时候允许接收报文，基于队列模板可配。具体选择有：
 基于字节统计的队列深度值低于丢弃阈值；
 队列深度低于队列尾丢弃阈值；
 队列深度低于队列尾丢弃阈值的x/8，x具体值在队列模板可配。

为了合理的利用队列资源，需要遵循的原则是尽量共享队列资源。通过拥塞管理，希望达到队列在芯片拥塞时，能够申请的缓存资源较少，反之能申请较多的缓存资源。
这里所说的队列资源是指报文的缓存资源。芯片对缓存资源的管理基即于CELL数目、也基于包数目管理，基于16BYTE级资源管理，允许预借资源。即每次判断资源数目时，用已占用资源数目进行判断。如果可以入队，则此报文必须能够全部入队，避免报文头可以接收，报文身体丢弃的决策。因为支持预借，这种情况下，芯片共享缓存需要再保留一些资源给预借，即此时队列独享资源，共享队列资源之和必须小于芯片支持的最大队列资源数目。
队列缓存管理需要明确几个概念。
首先是状态信息的一些名词:
 q_depth:队列占缓存资源的深度
 group_depth:队列组占共享缓存资源的深度
 chip_depth:芯片占共享缓存资源的深度
下面是一些配置信息:
 q_private:保留给队列的独享缓存资源
 q_max:队列最大可用缓存数目
 q_cong:队列拥塞门限。队列占用缓存数目大于等于此值时；否则，认为队列不拥塞。
 group_max:队列组最大可用共享缓存数目
 chip_drop_group:基于芯片拥塞的队列组丢弃门限。即芯片占共享缓存超过此阈值时，丢弃相应队列组的报文。
 group_cong:队列组占用共享缓存数目大于等于此值时；否则，认为队列组不拥塞。
 chip_cong:芯片的拥塞阈值。即芯片占共享缓存资源超过此值认为芯片已经拥塞。
下面是具体的包丢弃管理方案



内外置缓存的选择
自适应的动态选择内置或者外置的算法如下：
出端口组的绑定关系见寄存器NNI_GP，ETH_GP，PIE_GP，PRBS_GP，WOE_GP。
出端口组的水线配置见寄存器PORT0_GP_TH，PORT1_GP_TH，PORT2_GP_TH，PORT3_GP_TH，FREE_CNT_TH。

具体的缓存选择功能如下：
通过FAM送过来的4组出端口的IFB cell统计值，判断报文所属的出端口是否有足够的资源支持报文存放到IFB中，如果资源不足，则存放到EFB中；Jumbo帧固定走内置缓存，无足够的内置缓存时丢弃报文；出端口总共有4组，对应的优先级别从port0～port3递减，使用4个出端口IFB资源使用统计，port_cnt0~port_cnt3。
各个出端口组的要求如下：
对于port3要求： port0_cnt+port1_cnt+port2_cnt+port3_cnt <=port3_gp_th才能走IFB通道；
对于port2要求： port0_cnt+port1_cnt+port2_cnt <=port2_gp_th才能走IFB通道；
对于port1要求： port0_cnt+port1_cnt <=*port1_gp_th才能走IFB通道；
对于port0要求： port0_cnt <=port0_gp_th才能走IFB通道；

Internal Loop
为了提供用户级的Qos，需要层次化的调度拓扑，EQM 采用二次入队的方法来实现这个功能。PE 编辑后的报文进入EQM 的队列，如果二次入队指示有效，则经过拥塞管理、调度然后从PORT0 端口调度出队后进入EPS，EPS 读取二次入队所需要的队列信息后再环回到EQM 的入端口，再重新经过拥塞管理、调度发往真正的出端口。
功能示意图如下：

Packet Data Manager
PDM 子模块根据报文的转发出端口 进行分组管理，可以配置上行报文使用EFB 缓存
报文，下行报文使用IFB 缓存报文，这样可以减少上行报文因为写DDR 的时延大而
阻塞下行的要缓存到IFB 的报文，可以更充分地利用IFB 的带宽，减轻EFB 的带宽负
荷。
 PDM 子模块负责EFB 缓存地址的申请使用。当报文选择缓存到EFB 时，PDM 向
FAM申请EFB CELL 地址， PDM 模块将报文数据写入相应的CELL 地址中，同
时完成帧内链的链接。
 PDM 子模块决策报文是缓存在IFB 还是EFB， 有两种模式选择内外置缓存，采用
哪种模式是配置决定的。模式1（自适应模式）：根据 端口组IFB 缓存的占用情况，优先使用IFB，IFB 超过端口组的IFB 占用水线时使用EFB 缓存报文；模式2
（强制模式，仅作为Debug 使用）： 根据报文的出端口和通道组配置，将报文分
成4 个组，按组固定配置使用IFB 或EFB；
 如果报文选择缓存在EFB，则PDM 模块将报文数据从PLB 搬到EFB，然后将FD
送往QE 子模块入队。如果报文选择缓存在EFB，则不需要搬动报文，只需要将报
文的所占的PLB 资源统计转为IFB 资源统计，然后将FD 送往QE。
 当配置使用自适应模式缓存选择，出端口总共有4 组，对应的优先级别从port0～
port3 递减，使用4 个出端口IFB 资源使用统计，port_cnt0~port_cnt3 另外还有一个
组播身体专用的cell 使用资源统计mc_cnt。
 当报文为单播报文的时候，各个出端口组的要求如下：
 对于port3 要求：mc_cnt+port0_cnt+port1_cnt+port2_cnt+port3_cnt <=port3_gp_th 才
能走IFB 通道
 对于port2 要求：mc_cnt+port0_cnt+port1_cnt+port2_cnt <=port2_gp_th 才能走IFB
通道；
 对于port1 要求：mc_cnt+port0_cnt+port1_cnt <=*port1_gp_th 才能走IFB 通道；
 对于port0 要求：mc_cnt+port0_cnt <=port0_gp_th 才能走IFB 通道；

Schedule Output
 EQM 的调度输出支持基于逻辑通道的反压,PE 的入队和EPS 的二次入队请求首释
放通道FIFO 的将满反压。逻辑通道EQM_CHID 的具体含义见错误!未找到引用源。
EQM _CHID 说明章节。
 EPS 需要周期把逻辑通道的状态送给EQM，EQM 把它映射到调度器的输出。受反
压的调度器输出不能送给EPS。
 EPS 送给EQM 的反压状态有一定的延时。因此，允许反压产生之后，EQM 还送
出1~2 个帧描述符FD 给EPS。
 同一个端口，最快16 拍送出一个FD；
 不同端口之间，最快可以8 拍输出一个FD；

Port Schedule
逻辑支持4 个端口调度器。为了节省模块间的布线资源，需要时分复用一组总线，把调度
结果送给后序模块。端口之间进行RR 轮询。

Shaper
 EQM 的实体队列、调度器的输出都有shaper 进行限速。
 SHAPER 支持预借令牌。最多预借一个报文的令牌。
 基于实体队列的shaper:每个队列可配绑定支持一个shaper，即同时支持的shaper
数目跟实体队列数目一致。当shaper 令牌不够时，相当于一级调度器看不到此实
体队列的调度请求。
 通用调度器的shaper:每个通用调度器可配绑定支持一个shaper，即同时支持的
shaper 数目跟通用调度器的数目一致。当shaper 令牌不够时，相当于通用调度器没
有调度结果。

Schedule Algorithm
EQM 同时支持RR、 SP/WRR 等调度算法。下面详细解释各个算法的特点和用法。
Strict Priority：
Strict Priority(SP)调度即严格优先级调度，是基于优先队列（Priority Queuing，PQ）
的一种调度算法。该算法按照队列优先级的高低进行调度，高优先级先调度，低优先级后
调度.高实体队列非空，就先调度优先级高的队列出队，只有在优先级高的队列调度空后，
才调度低实体队列。如下图所示，从Queue0 到Queue3 也按照优先级的降序做绝对优先
调度，在报文出队的时候，SP 首先让处于最高优先级的Queue0 队列中的报文出队，直到
Queue0 队列中的报文发送完，然后发送中优先队列中的报文，同样直到发送完，依次类
推，只有Queue0、Queuel 和Queue2 队列的报文全部出队才去调度Queue3 队列。只要高
实体队列非空就一直调度高实体队列， 优先级级别越低的队列得到调度机会就越少
(Queue0>Queuel>Queue2>Queue3)。
Round Robin：
Round Robin (RR),也就是轮流调度各个输入端。主要适用于对帧长不敏感，各个输入端权
重一样的调度。
Weight Round Robin：
WRR 调度要求定义一个数值用于规定当前队列与其他优先级队列的相对重要性
（weight）。WRR 调度防止低优先级的队列在高优先级队列传输时被完全忽略。WRR 调
度对各个队列实行轮流发送机制。报文的权重与队列的重要性相对应。通过调度功能，即
使高优先级的队列为非空，低优先级的队列也能获得机会发送报文，这样带宽资源可以得
到充分的利用。
SP/WRR:
SP/WRR 是SP 和WRR 混合调度，如下图所示。对每个输入端可配组属性及WRR 调度权
重。逻辑对同一组的输入端进行WRR 调度，然后对各个组进行SP 调度。进行SP 调度时
候，0 优先权最高，7 优先权最低。

Share-Bandwidth Schedule
共享带宽设计需要支持的目标：
支持用户间的带宽保障
支持用户业务间的带宽保障。
支持用户间高优先级业务的带宽保证。
场景1（多用户带宽保证）：
总上行带宽20Mbps。
如果只有用户A 上网，则A 独占20Mbps 带宽。
如果用户B 加入，则A/B 各分配最低5Mbps 保证带宽，A 优先级比B 高，可以优先级占
用10Mbps 的保留带宽，A 最高15Mbps 最大带宽。
如果A 和B 同时在线，则A/B 各分配最低5Mbps 保证带宽，则剩余的10Mbps 带宽可以
按比例分配给A 和B。
场景2（多业务带宽保证）：
总上行带宽20Mbps。
如果用户只有上网业务，则独享20Mbps 带宽。
如果用户有视频和上网业务，视频和上网各配置5Mbps 的保证带宽，则视频优先级高可
以分配到15Mbps 带宽，上网分配5Mbps。
如果用户有视频和上网业务，视频和上网各配置5Mbps 的保证带宽，则剩余的10Mbps 带
宽可以按比例分配给视频和上网。
场景3（多用户多业务带宽保证）：……
用户A 在玩即时游戏，用户B 在做FTP 下载。
则用户A 优先使用带宽，用户B 低优先级使用带宽。
QoS 需要支持如上图所示的拓扑结构：用户业务队列，用户队列，和端口优先级队列。
从外到内，
 第一级为端口优先级队列，支持4/8个业务优先级。
 第二级为用户队列。
 第三级为用户业务队列，每个用户4/8个优先级队列。

A.保证带宽令牌桶：
各种业务的保证令牌根据配置的xir 配置的大小，按照固定的125us 周期往令牌桶里添加
令牌。如果业务流量少于保证带宽，令牌桶慢慢就会被填满，周期性添加令牌时就会有溢
出，溢出部分的令牌就会被添加到共享令牌桶中。
B.共带宽令牌桶：
共享保证令牌桶根据配置的保留带宽配置的大小，按照固定的125us 周期往令牌桶里添加
令牌。如果各个保证带宽令牌有溢出，溢出部分的令牌会加上周期性添加的令牌一起添加
到共享令牌桶中。
调度:
在有保证带宽令牌的情况下，优先使用保证带宽令牌桶里的令牌，消耗共享令牌阶段采用
RR 调度，确保各业务流的保证带宽能实施。
在调度器各个输入的保证带宽都消耗完或队列空的情况下，根据保留带宽的分配策略来调度；若按照优先级来分配保留带宽则此阶段进行SP调度，若按照比例来分配保留带宽，则此阶段进行WRR调度。

1. 仅0~7号通用调度器支持共享带宽特性；耗不完的保证带宽会转为共享带宽，共享带宽的根据配置采用SP调度或RR调度。
2. 将GSSA表中的specific_sch_sel配置成2‘b10，共享带宽按照优先级分配，调度器输入端抢占共享带宽的优先级在GSSA表中配置。
3. 将GSSA表中的specific_sch_sel配置成2‘b11，共享带宽按照比例权重分配，各输入端的抢占共享带宽权重在STS表中配置。
4. 使用该共享带宽调度时，调度器各个输入端的保证带和共享带宽的BWT表中的bw_ctrl_en配置成1；
5. 使用共享带宽特性的调度器仅支持0~7 8个输入端。
6. 保证带宽的配置在BWT表中0~63的表项中配置，譬如调度器0的输入2的保证带宽是在BWT的地址0x02上配置；调度器3的输入1的保证带宽是在是在BWT的地址0x31上配置；
7. 0~7共享带宽的配置在BWT表中地址为64~71的表项中配置，譬如调度器0在BWT的地址64上配置；调度器7的共享带宽是在BWT的地址71上配置；
8. 使用共享带宽特性的调度器仅支持0~7 8个输入端。
9. 保证带宽的XBS建议配置成2倍的速率配置值，譬如保证带宽10Mbps,则XBS建议配置成MAX {MTU，2*10_000/64 Byte}。
10. 共享带宽的XBS至少需要配置成调度器总带宽的匹配值的2倍，譬如调度器有4个输入端，每个输入端的保证带宽为20Mbps,共享带宽为10Mbps, 则共享带宽的XBS至少需要配置成MAX{MTU，2* 90000/64 Byte}。

1. 多级调度时，仅支持其中一级调调度器支持共享带宽特性，同一调度收敛树上不支持多级调度同时使用共享带宽特性。
2. 使用共享带宽特性的调度器不支持传统的shaping功能。(调度器的输入支持shaping, 调度器的输出不支持shaping)
